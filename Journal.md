# Week Three Journal

I've had a really busy few weeks. At work I've had an increased work load and during this time my productivity has taken a serious hit. On top of that it has been busy lately where I work my second job and I've had to fill in for a few delivery drivers during the week which has not helped my time.

I also faced some minor errors which frustrated me and made me stop working on it for the night. An example being unable to more or less use regex to format the data, or when I tried to load my cleaned data into Gephi. These minor frustrations halted my progress throughout, and coupled with my other busyness resulted in me submitting week three's reflection late. This also means that Week 4 will probably be late for me, but next week I believe I have things planned out to ensure I can finish Week 4's material and Week 5 by the normal deadline.


## Readings

### Soft Digital History - [https://cradledincaricature.com/2017/05/24/the-soft-digital-history-that-underpins-my-book/](https://cradledincaricature.com/2017/05/24/the-soft-digital-history-that-underpins-my-book/)

### Hard Digital History - [https://cradledincaricature.com/2017/06/06/the-hard-digital-history-that-underpins-my-book/'](https://cradledincaricature.com/2017/06/06/the-hard-digital-history-that-underpins-my-book/')

I found these articles very interesting, and a really good fit for this course. They discuss how a lot of the work we do for projects isn't visible in the final product, but they have a drastic impact on how we think about that project. It shows us some real examples of why it is a good idea to document everything we do, since everything we do is important work, and not just the final product. 

Reading these made me see why it is important that we document our process. I was always one to focus on doing whatever was necessary to submit the clean final version of something and ignoring all of the miscellaneous work that didn't fit in. But this course has shown me that it is important for our discarded or temporary work to exist as well. 

I found the distinction he made between the soft history and the hard history interesting, especially in the context of a lot of digital humanities work striving to be as open as possible. Having to struggle between releasing your findings as a whole with no narrative, or almost cherry-picking some results to tell a narrative is an interesting quandry. It seems that these articles came out of the author grappling with this. These articles really made me think about how historians collectively approach projects and the opened my eyes to some of the nuances involved.


## REGEX

I thought I was fairly comfortable using REGEX, since I like when things follow rules and operate accordingly, however I faced some challenges that I did not anticipate. There weren't any major issues that I could've used help on I don't think, but instead it was just little things that I think I was doing wrong that resulted in my text being very different from what the end result should have looked like. This was frustrating, but when I went back later and sat down with a clear head on worked on it again I was able to get it to where I thought it was supposed to be. 

One aspect I faced difficulty with was making decisions on what to clean. While going through the document I saw many errors that I could easily fix by hand, but the point of this exercise was doing things by hand, and not wanting to interpret something wrong.

I faced similar challenges while working at the Library and Archives last summer. As an ATIP Clerk, we were tasked with registering new requests, so we had to copy what the requestors inputted, even if we knew it was likely wrong. We had to make judgements with addresses, names, etc. Sometimes things were obvious typos that we were fairly confident with, but othertimes it was more of a grey area where we had to make a judgement.

For this exercise specfically, it would have been easy to go line by line and fix some of the more obvious errors I had found, but then that creates a bad precedence, and as mentioned in the instructions, some days the person making the corrections values things differently resulting in different data. It made me think about the articles last week, and how the crowd-sourced transcriptions were handled. There were clear guides, and an oversight process to ensure that they could be as accurate as possible.


## Open Refine

Open Refine was fun to play with to clean the data. Again, I faced the challenge of cleaning the data and deciding which names could be amalgamated. This tool seems to have a lot of benefit for cleaning and sorting data, and I am looking forward to seeing what else it can do.

## Gephi

For some reason I already had Gephi installed on my computer but I had not used it before, not sure why but it save me from having to download it again! At the beginning it wouldn't open for me, but I saw others faced similar issues on discord so I followed the steps they took to fix it and I got everything working properly.

I faced some annoyance with Gephi and trying to open my own cleaned dataset. It kept giving me errors as seen in my [Notes](Notes.md) this week. I thought I had tried using the clean dataset provided but I guess I hadn't and when I loaded the correct version everything seemed to work properly. I had fun messing around with some of the options and when I have more time to see what else the software can do.

I did a quick preview and its obvious that I do need to spend more time playing with it [GephiTestPreview.pdf](GephiTestPreview.pdf).

With all the various things the software can do, it would be interesting to see how many finished projects looked different using different weights or different filters. 


## Weekly Reflection

